For T iterations of K-means, and using specified features, there were M misclassified data points, out of a total of 178.
T = 1, features = [7:10], M = 47
T = 2, features = [7:10], M = 47
T = 5, features = [7:10], M = 44
T = 10, features = [7:10], M = 44
T = 20, features = [7:10], M = 44

Fixing T = 10, features = [7:10], and changing the random initialization of the centroids, we obtain various local minima for the objective function.
M varies in the range 42-62.

Fixing T = 10, features = [1:5,6:13], and changing the random initialization of the centroids, we obtain various local minima for the objective function.
M varies in the range 42-51.



For T iterations of EM-GMM, and using specified features, there were M misclassified data points, out of a total of 178.
T = 10, features = [7:10], M = 80
T = 20, features = [7:10], M = 78
T = 50, features = [7:10], M = 41
T = 100, features = [7:10], M = 40

Fixing T = 10, features = [1:5,6:13], and changing the random initialization of the cluster priors and likelihood priors, 
we obtain various local minima for the objective function.
M varies in the range 17-45.



Conclusions:
- for optimal tuning of the meta-parameters and initialization data, EM-GMM outperforms K-means, with 9.5% versus 23.5% misclassification rate
- EM-GMM converges more slowly than K-means
- EM-GMM is more sensitive to the selected features than K-means